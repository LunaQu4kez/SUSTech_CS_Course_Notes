{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster\n",
    "\n",
    "\n",
    "This notebook provides a comprehensive guide on implementing Cluster using scikit-learn libraries.\n",
    "\n",
    "By: Bryce Li, NUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering text documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example showing how the scikit-learn API can be used to cluster documents by topics using a Bag of Words approach.\n",
    "\n",
    "Two algorithms are demonstrated, namely KMeans and its more scalable variant, MiniBatchKMeans. Additionally, latent semantic analysis is used to reduce dimensionality and discover latent patterns in the data.\n",
    "\n",
    "This example uses two different text vectorizers: a TfidfVectorizer and a HashingVectorizer. See the example notebook FeatureHasher and DictVectorizer Comparison for more information on vectorizers and a comparison of their processing times.\n",
    "\n",
    "For document analysis via a supervised learning approach, see the example script Classification of text documents using sparse features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load data from The 20 newsgroups text dataset, which comprises around 18,000 newsgroups posts on 20 topics. For illustrative purposes and to reduce the computational cost, we select a subset of 4 topics only accounting for around 3,400 documents. See the example Classification of text documents using sparse features to gain intuition on the overlap of such topics.\n",
    "\n",
    "Notice that, by default, the text samples contain some message metadata such as \"headers\", \"footers\" (signatures) and \"quotes\" to other posts. We use the remove parameter from fetch_20newsgroups to strip those features and have a more sensible clustering problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3387 documents - 4 categories\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying the quality of clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define a function to score different clustering pipelines using several metrics.\n",
    "\n",
    "Clustering algorithms are fundamentally unsupervised learning methods. However, since we happen to have class labels for this specific dataset, it is possible to use evaluation metrics that leverage this “supervised” ground truth information to quantify the quality of the resulting clusters. Examples of such metrics are the following:\n",
    "\n",
    "* homogeneity, which quantifies how much clusters contain only members of a single class;\n",
    "\n",
    "* completeness, which quantifies how much members of a given class are assigned to the same clusters;\n",
    "\n",
    "* V-measure, the harmonic mean of completeness and homogeneity;\n",
    "\n",
    "* Rand-Index, which measures how frequently pairs of data points are grouped consistently according to the result of the clustering algorithm and the ground truth class assignment;\n",
    "\n",
    "* Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster assignment have an ARI of 0.0 in expectation.\n",
    "\n",
    "If the ground truth labels are not known, evaluation can only be performed using the model results itself. In that case, the Silhouette Coefficient comes in handy. See Selecting the number of clusters with silhouette analysis on KMeans clustering for an example on how to do it.\n",
    "\n",
    "For more reference, see Clustering performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering on text features\n",
    "\n",
    "Two feature extraction methods are used in this example:\n",
    "\n",
    "* TfidfVectorizer uses an in-memory vocabulary (a Python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus.\n",
    "\n",
    "* HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-sphere) which seems to be important for k-means to work in high dimensional space.\n",
    "\n",
    "Furthermore it is possible to post-process those extracted features using dimensionality reduction. We will explore the impact of those choices on the clustering quality in the following.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorization done in 0.231 s\n",
      "n_samples: 3387, n_features: 7929\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After ignoring terms that appear in more than 50% of the documents (as set by max_df=0.5) and terms that are not present in at least 5 documents (set by min_df=5), the resulting number of unique terms n_features is around 8,000. We can additionally quantify the sparsity of the X_tfidf matrix as the fraction of non-zero entries divided by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering sparse data with k-means\n",
    "\n",
    "As both KMeans and MiniBatchKMeans optimize a non-convex objective function, their clustering is not guaranteed to be optimal for a given random init. Even further, on sparse high-dimensional data such as text vectorized using the Bag of Words approach, k-means can initialize centroids on extremely isolated data points. Those data points can stay their own centroids all along.\n",
    "\n",
    "The following code illustrates how the previous phenomenon can sometimes lead to highly imbalanced clusters, depending on the random initialization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this problem, one possibility is to increase the number of runs with independent random initiations n_init. In such case the clustering with the best inertia (objective function of k-means) is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All those clustering evaluation metrics have a maximum value of 1.0 (for a perfect clustering result). Higher values are better. Values of the Adjusted Rand-Index close to 0.0 correspond to a random labeling. Notice from the scores above that the cluster assignment is indeed well above chance level, but the overall quality can certainly improve.\n",
    "\n",
    "Keep in mind that the class labels may not reflect accurately the document topics and therefore metrics that use labels are not necessarily the best to evaluate the quality of our clustering pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
